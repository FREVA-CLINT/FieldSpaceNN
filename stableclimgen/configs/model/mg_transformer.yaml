_target_: stableclimgen.src.models.mg_transformer.pl_mg_model.LightningMGModel
lambda_loss_dict:
  zooms:
    MSE_loss: 1
weight_decay: 0
lr_groups:
  default:
    lr: 5e-4
    warmup: 0
  attention:
    matches: ['TransformerBlock','GridSelfAttention']
    lr: 5e-4
    warmup: 500
model:
  _target_: stableclimgen.src.models.mg_transformer.mg_transformer.MG_Transformer
  mgrids: ${mgrids}
  mg_emb_confs: ${embedding.MGEmbedder.mg_emb_confs}
  in_zooms: [3,4,6]
  in_features: 1
  out_features: 1
  lift_features: 1
  n_head_channels: 15
  learn_residual: false
  use_mask: true
  layer_confs_emb:
    n_groups: 1
    aggregation: 'shift_scale'
  block_configs:
    "input":
      _target_: stableclimgen.src.modules.multi_grid.confs.MGProcessingConfig
      out_features: [128,128,128]
      out_zooms: [3,4,6]
      layer_settings:
        type: 'linear'
        embed_confs:
          embed_names: ['DensityEmbedder']
          embed_confs: ${embedding}
          embed_mode: "sum"
    "0":
      _target_: stableclimgen.src.modules.multi_grid.confs.MGProcessingConfig
      out_features: [128,128,128]
      zooms: [3,4,6]
      layer_settings:
        type: "TransformerBlock"
        blocks: ["s","mlp","vsnh","mlp"]
        seq_lengths: [10,3,3]
        n_head_channels: 32
        mlp_mult: 2
    "8":
      _target_: stableclimgen.src.modules.multi_grid.confs.MGSelfProcessingConfig
      out_features: 128
      q_zooms: [3,4,6]
      kv_zooms: [3,4,6]
      layer_settings:
        with_nh: true
        common_kv: false
        n_head_channels: 32
        mlp_mult: 2
    "output":
      _target_: stableclimgen.src.modules.multi_grid.confs.MGProcessingConfig
      out_features: [1,1,1]
      zooms: [3,4,6]
      layer_settings:
        type: 'mlp'